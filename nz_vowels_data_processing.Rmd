Using an Rmd file for its excellent ability to identify the working directory automatically without having to set up an Rproject.


```{r}
library(tidyverse)
library(phonTools)
library(gganimate)
library(mgcv)
library(ggforce)
library(parallel)
library(emuR)

###

price_wide <- read_csv("raw_data/price.csv")

# removing some non-essential columns, renaming columns
# positive select statement will help with generalisability to other data sets
price_wide <- price_wide %>% 
  dplyr::select(`Number`, `Speaker`, `Corpus`, `participant_gender`, `participant_year_of_birth`, `Before Match`, `Text`, `After Match`, `syllables/sec`, `Target phonemic transcription`, `Target lemma`, `Target words since last mention`, `Target last wordform instance same speaker (sec. ago)`, `Target frequency (celex lemma)`, `Target freq (celex wordform)`, `Target orthography`, `Match segments`, `Target segments`, `Target segments start`, `Target segments end`, matches("(time|F[1-3]|error).*")) %>%
  select_all(tolower) %>%
  select_all(function (x) {gsub("[ -]", "_", x)}) %>%
  select_all(function (x) {gsub("/", "_per_", x)}) %>%
  select_all(function (x) {gsub("[()]", "", x)}) %>%
  select_all(function (x) {gsub("_sec._ago", "", x)}) %>%
  select_all(function (x) {gsub("target_", "", x)}) %>%
  rename(sex=participant_gender,
         yob=participant_year_of_birth,
         phon=phonemic_transcription,
         celex_lemma=frequency_celex_lemma,
         celex_wf=freq_celex_wordform,
         wordform=orthography) %>%
  mutate(dur=segments_end - segments_start) %>%
  dplyr::select(-`segments_start`, -`segments_end`)

# now move to long format & filter out NA's

price <- price_wide %>%
  reshape(varying=list(paste0("time_", c(paste0("0.", 0:9), "1.0")),
                       paste0("f1_time_", c(paste0("0.", 0:9), "1.0")),
                       paste0("f2_time_", c(paste0("0.", 0:9), "1.0")),
                       paste0("f3_time_", c(paste0("0.", 0:9), "1.0"))),
          v.names=c("time", "f1", "f2", "f3"),
          timevar="measurement_no",
          direction="long") %>% # fuck you, gather()!
  dplyr::select(-number) %>% 
  mutate(id_str=paste0("price_", id)) %>%
  group_by(id) %>%
  # also filtering entire vowels with too many NA's
  filter(!(sum(is.na(time)) > 4 | sum(is.na(f1)) > 4 | sum(is.na(f2)) > 4)) %>% 
  ungroup() %>%
  arrange(id, measurement_no) %>%
  mutate(id=id_str) %>% dplyr::select(-`id_str`)
  
# creating a PDF for checking formant tracking by speaker
if (FALSE) {
  pdf("checking_price.pdf", onefile=T, width=7.3, height=10)
  speakers <- unique(price$speaker)
  for (i in seq(1,length(speakers),70)) {
    small <- filter(price, speaker %in% speakers[i:min(i+69,length(speakers))])
    p<- ggplot(small, aes(x=measurement_no, y=f1, group=id)) +
      facet_wrap(~speaker, nrow=10, ncol=7) +
      geom_line(col="red", alpha=0.5) +
      geom_line(aes(y=f2), col="blue", alpha=0.2)
    print(p)
  }
  dev.off()
}

# speaker exclusion list
# based following criteria: exclude if...
#   - massive variation in f2 with no observable trend similar to individuals from same period
#   - consistent formant switching errors (f2 -> f1)
#   - very few tokens
# all of this is based on PRICE (the assumption being that formant tracking errors result from
# poor recording quality / idiosyncracies of a given speaker, and should therefore be similar
# across all vowels)
exclude_speakers <- c("Ada Aitcheson", "Anna Hayes", "Annie Hamilton", "Catherine King", "Christina Bisset", "Edith German", "Henry Swarbrick", "Hannah Cross", "Jessie Drinnan", "Jane Reid", "Marguerite Symons", "Annette Golding", "Elizabeth Arnott", "Jean Atkinson", "John Barron", "Ken Algie", "Mavis Jackson", "Nan Hay", "Sydney Farrell", "Rupert Pyle", "Ruth Greer", "Vera Hayward", "fyp09−6", "mon09−3", "mon97−19a", "mon96−2a", "mon99−1b", "mon99−13b", "mop02−5", "mop98−18", "myn97−6a", "myn97−9", "myp00−18a", "myp00−18a", "myp00−18a", "myp07−1a", "myp07−6", "myp94−17", "myp09−2", "myp94−8c", "myp98−12b")

# also excluding partial words and words that don't contain PRICE (there are a fair few!)
price_filtered <- price %>%
  filter(!(speaker %in% exclude_speakers),
         !grepl("~", wordform),
         grepl("2", phon))


# environments

# function to find segments in wordform by specifying position relative to 
# target segment; 0=segment location, -1=preceding, +1=following
find_context <- function (text, segment, where) {
  locations <- as.vector(regexpr(segment, text))
  lengths <- nchar(text)
  context_locations <- locations + where
  contexts <- str_sub(text, context_locations, context_locations)
  contexts[context_locations <= 0] <- "none"
  contexts[context_locations > lengths] <- "none"
  return(contexts)
}

# add predictor coding for following voicing
price_filtered <- price_filtered %>%
  mutate(previous=find_context(phon, "2", -1),
         following=find_context(phon, "2", 1),
         following_voiceless=following %in% c("p","t","k","J","f","T","s","S","h"))

  

# fixing trajectories!
# 1) certain values are just unreasonable for f1 / f2 (depending on male / female)

# let's use American English vowel data to estimate reasonable / unreasonable f1 / f2
# (from phonTools)

data(h95)

# ggplot(h95, aes(x=f1)) + facet_wrap(~type) + geom_density()
  
f_ranges <- h95 %>%
  group_by(type) %>%
  summarize(f1_lower=quantile(f1, 0.01),
            f1_upper=quantile(f1, 0.99),
            f2_lower=quantile(f2, 0.01),
            f2_upper=quantile(f2, 0.99)) %>%
  ungroup() %>%
  mutate(sex=recode(type, m="M", w="F")) %>%
  dplyr::select(-type)

not_outlier <- function (x) {
  x > (quantile(x, 0.25) - 1.5*IQR(x)) & x < (quantile(x, 0.75) + 1.5*IQR(x))
}

price_filtered <- price_filtered %>%
  # filtering based on reasonable ranges / sex
  inner_join(f_ranges, by="sex") %>%
  filter(!is.na(f1) & !is.na(f2)) %>%
  # marking clear f1/f2 outliers for each speaker as NAs
  group_by(speaker, measurement_no) %>%
  mutate(f1=ifelse(f1 > f1_lower & f1 < f1_upper & not_outlier(f1), f1, NA),
         f2=ifelse(f2 > f2_lower & f2 < f2_upper & not_outlier(f2), f2, NA)) %>%
  ungroup() %>%
  group_by(id) %>%
  # once again, removing trajectories with too many NAs
  filter(sum(is.na(f1)) < 5 & sum(is.na(f2)) < 5) %>%
  ungroup() %>%
  filter(!is.na(yob), !is.na(f1), !is.na(f2))

# save as RDS
saveRDS(price_filtered, "final_data/price_full.rds")

# creating averaged data (male AND female)
price_filtered_avgs <- price_filtered %>%
  group_by(speaker, following_voiceless, measurement_no) %>%
  summarise(f1_avg=mean(bark(f1)),f2_avg=mean(bark(f2)),f3_avg=mean(bark(f3)),
            dur_avg=mean(dur),
            yob=yob[1], sex=sex[1]) %>%
  ungroup() %>%
  # for GAMMs again...
  mutate(AR_start=measurement_no==1)

# save as RDS
saveRDS(price_filtered, "final_data/price_averaged.rds")
```
